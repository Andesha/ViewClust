from datetime import datetime
import numpy as np
import pandas as pd
from viewclust.target_series import target_series


def job_use(jobs, d_from, target, d_to='', use_unit='cpu', job_state='all',
            time_ref='', serialize_queued='', serialize_running='',
            serialize_dist=''):
    """Takes a DataFrame full of job information and
       returns usage based on specified unit.

    This function operates as a stepping stone for plotting usage figures
    and returns various series and frames for several different uses.

    Parameters
    -------
    jobs: DataFrame
        Job DataFrame typically generated by the ccmnt package.
    use_unit: str, optional
        Usage unit to examine.
        One of: {'cpu', 'cpu-eqv', 'gpu', 'gpu-eqv','gpu-eqv-cdr'}.
        Defaults to 'cpu'.
    job_state: str, optional
        The job state to include in measurement:
            {'all','complete', 'running', 'queued'}.
        Defaults to 'complete'.
    time_ref: str, one of: {sub, req, sub+req}
        sub: Jobs run as if they ran at submit time.
        req: Jobs run their full execution time at normal start time.
        sub+req: Jobs run their full execution time from their submit time.
    insta_dur: str, optional
        The job duration used to calculate time_ref One of: {'run', 'req'}.
        Defaults to 'run'.
    d_from: date str
        Beginning of the query period, e.g. '2019-04-01T00:00:00'.
    target: int-like
        Typically a cpu allocation or core eqv value for a particular acount.
        Often 50.
    d_to: date str, optional
        End of the query period, e.g. '2020-01-01T00:00:00'.
        Defaults to now if empty.
    debugging: boolean, optional
        Boolean for reporting progress to stdout. Default False.
    serialize_running, serialize_queued, serialize_dist: str, optional
        Pickles given structure with argument as a name.
        If left empty, pickle procedure is skipped.
        Defaults to empty.

    Returns
    -------
    clust:
        Frame of system info at given time intervals.
        Typically referenced by other functions for plotting information.
    queued:
        Frame of queued resources
    running:
        Frame of running resources
    dist_from_target:
        Series for delta plots
    """

    # d_to boilerplate
    # IF D_TO IS EMPTY IT SHOULD BE SET TO LATEST KNOWN STATE
    # CHANGE TIME (SUBMIT,START,END) IN THE JOB RECORDS.
    if d_to == '':
        t_max = jobs[['submit', 'start', 'end']].max(axis=1)
        d_to = str(t_max.max())

    # Filter on job state. Different from reason so it is safe.
    if job_state == 'complete':
        jobs_complete = jobs.copy()
        jobs_complete = jobs_complete.loc[jobs_complete['end'].notnull()]
        jobs = jobs_complete
    elif job_state == 'running':
        jobs_running = jobs.copy()
        jobs_running = jobs_running.loc[jobs_running['state'] == 'RUNNING']
        jobs = jobs_running
        jobs['end'] = jobs['start'] + jobs['timelimit']
    elif job_state == 'queued':
        jobs_queued = jobs.copy()
        jobs_queued = jobs_queued.loc[jobs_queued['state'] == 'PENDING']
        jobs = jobs_queued

        jobs['start'] = jobs['submit']
        jobs['end'] = pd.to_datetime(d_to) + jobs['timelimit']

    # Boilerplate for time transformation
    if time_ref != '':
        jobs = jobs.copy()
        if time_ref == 'sub':
            end = jobs['submit'] + (jobs['end'] - jobs['start'])
            jobs['start'] = jobs['submit']
            jobs['end'] = end
        elif time_ref == 'req':
            jobs['end'] = (jobs['submit'] + jobs['timelimit'])
        elif time_ref == 'sub+req':
            end = jobs['submit'] + jobs['timelimit']
            jobs['start'] = jobs['submit']
            jobs['end'] = end

    jobs = jobs.sort_values(by=['submit'])

    if use_unit == 'cpu':
        jobs['use_unit'] = jobs['reqcpus']
    elif use_unit == 'cpu-eqv':  # TRESBillingWeights=CPU=1.0,Mem=0.25G
        jobs['mem_scale'] = (jobs['mem'] / 1024) * .25
        jobs['use_unit'] = jobs[['mem_scale', 'reqcpus']].max(axis=1)
    elif 'gpu' in use_unit:
        jobs['ngpus'] = jobs['reqtres'].str.extract(
            r'gpu=(\d+)').astype('float')
        if use_unit == 'gpu':
            jobs['use_unit'] = jobs['ngpus']
        elif use_unit == 'gpu-eqv':
            # Beluga and Graham:
            # TRESBillingWeights=CPU=0.0625,Mem=0.015625G,GRES/gpu=1.0,GRES/gpu=1.0
            jobs['cpu_scale'] = jobs['reqcpus'] * 0.0625
            jobs['mem_scale'] = (jobs['mem'] / 1024) * 0.015625
            jobs['use_unit'] = jobs[['cpu_scale', 'mem_scale', 'ngpus']].max(
                axis=1)
        elif use_unit == 'gpu-eqv-cdr':
            # Cedar: TRESBillingWeights=CPU=0.1667,Mem=0.03125G,GRES/gpu=1.0
            jobs['cpu_scale'] = jobs['reqcpus'] * 0.1667
            jobs['mem_scale'] = (jobs['mem'] / 1024) * 0.03125
            jobs['use_unit'] = jobs[['cpu_scale', 'mem_scale', 'ngpus']].max(
                axis=1)
        else:
            raise AttributeError('invalid GPU use_unit')
    elif use_unit == 'billing':
        jobs['billing'] = jobs['reqtres'].str.extract(
            r'billing=(\d+)').astype('float')
        if jobs['billing'].isnull().any():
            raise AttributeError('There is no "billing" string in the reqtres')
        else:
            jobs['use_unit'] = jobs['billing']
    else:
        raise AttributeError('invalid use_unit')

    jobs_submit = jobs.copy()
    jobs_submit.index = jobs_submit['submit']
    jobs_start = jobs.copy()
    jobs_start.index = jobs_start['start']
    jobs_end = jobs.copy()
    jobs_end.index = jobs_end['end']

    # Grouping by second resolution, and resampling for an hour after grouping
    queued = jobs_submit.groupby(pd.Grouper(freq='S')).sum()[
        'use_unit'].fillna(0).subtract(jobs_start.groupby(pd.Grouper(
            freq='S')).sum()['use_unit'].fillna(0), fill_value=0).cumsum()
    running = jobs_start.groupby(pd.Grouper(freq='S')).sum()[
        'use_unit'].fillna(0).subtract(jobs_end.groupby(pd.Grouper(
            freq='S')).sum()['use_unit'].fillna(0), fill_value=0).cumsum()
    # Resample to hour
    queued = queued.resample('H').mean()
    running = running.resample('H').mean()

    baseline = target_series([(d_from, d_to, 0)])
    queued = queued.add(baseline, fill_value=0)
    running = running.add(baseline, fill_value=0)

    # Target: If int, calculate it, else use the variable passed
    # (should be a series)
    clust = pd.DataFrame()
    if isinstance(target, int):
        clust = target_series([(d_from, d_to, target)])
    else:
        clust = target

    sum_target = np.cumsum(clust)
    sum_running = np.cumsum(running)

    # New workaround for job record problems
    sum_target = sum_target.loc[d_from:d_to]

    sum_running.index.name = 'datetime'
    sum_running = sum_running.loc[d_from:d_to]

    dist_from_target = (sum_running - sum_target)

    if serialize_running != '':
        running.to_pickle(serialize_running)
    if serialize_queued != '':
        queued.to_pickle(serialize_queued)
    if serialize_dist != '':
        dist_from_target.to_pickle(serialize_dist)

    return clust, queued, running, dist_from_target
